# K臂赌博机实验框架

## 项目概述

本项目实现了一个灵活的K臂赌博机（K-armed Bandit）模拟和实验框架。核心目标是研究和评估在特定**两阶段设定**下的不同决策策略：

1. **实验阶段 (T 轮)**: 代理（Agent）与赌博机交互 T 轮，目的是收集信息以识别哪个“臂”可能提供最高的平均奖励。
2. **承诺阶段 (N 轮)**: 基于实验阶段收集的信息，代理选择一个“最佳”臂，并在接下来的 N 轮中**仅**拉动这个选定的臂。

该框架旨在探索不同策略在经典的**探索-利用权衡 (Exploration-Exploitation Trade-off)** 中的表现，特别是在这种两阶段结构下如何有效地平衡信息收集和奖励最大化。项目计算**总后悔值 (Total Regret)** 作为评估策略性能的主要指标。

## 文件结构

- `bandit.py`: 定义 `KArmedBandit` 类，模拟具有 K 个臂的赌博机环境。支持多种奖励分布（截断正态分布、伯努利分布、Beta 分布），并能计算最优臂。
- `experiment.py`: 定义 `BanditExperiment` 类，负责管理整个两阶段实验流程。它协调实验阶段和承诺阶段的运行，记录选择的臂和获得的奖励，并计算总后悔值。
- `strategies.py`: 包含用于实验阶段和承诺阶段的各种策略实现。
  - **实验阶段策略 (`CombinedStrategies`)**: 包括经典的 ε-Greedy, UCB (Upper Confidence Bound), Thompson Sampling (Beta 先验), Softmax, 以及一些变种和混合策略。
  - **承诺阶段策略 (`CommitmentStrategies`)**: 包括选择经验最佳臂 (`best_empirical`), 选择被拉动次数最多的臂 (`most_pulled`), 以及基于置信下界的策略 (`confidence_based`)。
  - **动态探索策略 (`Dynamic...Strategy`)**: 实现了两种动态确定实验阶段长度 T 的策略 (`DynamicConfidenceBoundsStrategy`, `DynamicBayesianStrategy`)，它们试图根据统计显著性或后验概率在识别出最优臂后提前停止探索。 *注意：当前这些动态策略在 `strategies.py` 内部模拟探索循环，可能需要进一步集成到 `experiment.py` 以获得更自然的控制流。*
- `visualization.py`: 提供可视化功能，包括绘制单次实验的详细结果（如各臂选择次数、估计值 vs 真实值、累积奖励）和比较多种策略组合的平均后悔值。
- `optimization.py`: 使用 `Optuna` 库进行超参数优化。提供了一个工厂函数 `objective_factory` 来为不同策略创建优化目标（最小化平均后悔值），并运行优化过程。
- `main.py`: 项目的入口点。配置实验参数（T, N, k, 重复次数等），定义要测试的策略组合，运行单次实验示例、多策略比较和超参数优化，并调用可视化函数生成图表。
- `requirements.txt`: 列出了项目运行所需的 Python 库 (如 numpy, matplotlib, seaborn, optuna)。
- `README.md`: 项目说明文档 (本文档)。

## 核心功能

- **两阶段实验**: 明确区分实验 (T轮) 和承诺 (N轮) 阶段。
- **多种策略**: 实现并比较多种经典的赌博机算法及动态探索策略。
- **可视化**: 生成图表展示单次实验细节和多策略性能对比。
- **超参数优化**: 使用 Optuna 自动调整策略的关键参数以最小化后悔值。
- **灵活性**: 易于扩展，可以添加新的赌博机环境、策略或评估指标。

## 使用方法

1. **安装依赖**:

    ```bash
    pip install -r requirements.txt
    ```

2. **运行实验**:

    ```bash
    python main.py
    ```

    - 可以在 `main.py` 文件顶部修改实验参数 (T, N, k, num_runs, n_trials_optuna)。
    - 脚本将执行单次实验示例、多策略比较和（可选的）超参数优化，并将结果图表保存在项目根目录。

## 动态探索算法简述

本项目探索了两种动态调整探索阶段长度的算法，旨在根据统计证据提前结束探索，理论上可能比固定 T 轮更有效率，特别是当承诺阶段 N 很大时。

1. **基于置信区间的动态探索 (频率派)**: 持续更新每个臂奖励均值的置信区间。当某个臂的置信下界高于所有其他臂的置信上界时，认为已足够确信找到最优臂，停止探索。
2. **基于后验概率的动态探索 (贝叶斯派)**: 使用贝叶斯方法（通常假设 Beta-Bernoulli 模型）更新每个臂是最优臂的后验概率。当某个臂的后验概率超过预设阈值（如 1-δ）时，停止探索。

*实现说明: 如文件结构中所述，当前这些动态策略的探索循环是在 `strategies.py` 内部模拟的。*