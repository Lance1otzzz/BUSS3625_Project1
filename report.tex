\documentclass[6pt, a4paper]{ctexart} % 使用 ctexart 文档类以支持中文, 11pt 字号, A4纸张

\usepackage{amsmath} % 数学公式
\usepackage{amssymb} % 数学符号
\usepackage{graphicx} % 插入图片
\usepackage{geometry} % 设置页边距
\usepackage{float} % 控制浮动体位置 (如图表)
\usepackage{lipsum} % 用于生成占位文本 (如果需要)
\usepackage{hyperref} % 创建超链接 (可选)
\usepackage{subcaption} % 在导言区添加

% 设置页边距，有助于控制页数
\geometry{left=1.5cm, right=1.5cm, top=1.5cm, bottom=1.2cm}

% 定义一些常用命令 (可选)
\newcommand{\N}{\ensuremath{N}}
\newcommand{\T}{\ensuremath{T}}
\newcommand{\kArms}{\ensuremath{k}}
\newcommand{\muStar}{\ensuremath{\mu^*}}
\newcommand{\muI}{\ensuremath{\mu_i}}
\newcommand{\aT}{\ensuremath{a_t}}
\newcommand{\rT}{\ensuremath{r_t}}
\newcommand{\eps}{\ensuremath{\epsilon}} % 使用 epsilon 而非 varepsilon
\newcommand{\deltaVal}{\ensuremath{\delta}}
\newcommand{\DeltaVal}{\ensuremath{\Delta}}

\title{多臂赌博机两阶段框架下的策略分析}
\author{江木力}
\date{\today} 

\begin{document}
\maketitle

\section{算法策略分析}
不难发现算法可以是一个固定探索策略和一个承诺阶段策略的组合。一个比较朴素的想法是：探索阶段找若干个策略，承诺阶段找若干个策略，然后任意组合。在这份作业中，我们将使用这些策略作为baseline。

\subsection{固定探索策略}
\begin{itemize}
    \item \textbf{$\eps$-贪婪(Epsilon-Greedy)}: 以 $\eps$ 概率随机探索， $1-\eps$ 概率选择当前最优臂。
    \item \textbf{UCB(Upper Confidence Bound)}: 选择上置信界最高的臂，平衡探索与利用。
    \item \textbf{Thompson采样(Thompson Sampling)}: 基于Beta分布后验采样。
\end{itemize}

\subsection{承诺阶段策略}
\begin{itemize}
    \item \textbf{最佳经验值(BestEmpirical)}: 选择估计平均奖励最高的臂。
    \item \textbf{最常拉动(MostPulled)}: 选择被拉动次数最多的臂。
    \item \textbf{置信下界(LCB)}: 选择置信下界最高的臂，更保守的选择方法。
\end{itemize}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/comparison_regret_over_time_T100_N500_k20.png}
        \caption{长时间承诺}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/comparison_regret_over_time_T5000_N1000_k10.png}
        \caption{长时间探索}
        \label{fig:image2}
    \end{subfigure}
    \caption{不同时间长度的对比}
    \label{fig:time_comparison}
\end{figure}
\subsection{更新算法动机}
想法是很简单的：在利用阶段，我们非常希望利用最“好”的臂，因为选错的代价非常高昂，足足为$\mathcal{O}(N)$。反观探索阶段，我们的算法都是次线性的，因此我们希望探索阶段稍稍做一点让步。
但是如果探索阶段本身过长了，我们就会浪费掉很多时间去探索那些“次优”的臂。所以我们面对的是这样一个需求：探索阶段要充分探索，保证承诺阶段的结果够好；但也要保证探索阶段不能太长，不要浪费太多的付出。

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/single_run_regret_regular_UCB.png}
        \caption{直接粗暴组合策略}
        \label{fig:image3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/single_run_regret_dynamic_Hoeffding.png}
        \caption{尝试动态探索}
        \label{fig:image4}
    \end{subfigure}
    \caption{幻想中的优化}
    \label{fig:both_images}
\end{figure}

\section{改进策略理论分析}

这些策略的核心思想是在探索阶段\textbf{动态地决定何时停止探索}并进入承诺阶段，而不是预先固定探索轮数 $T$。它们都试图在有足够统计信心确定最佳臂时停止探索，并将置信度要求与承诺阶段的长度 $N$ 联系起来，因为 $N$ 越大，选错臂的累积损失（后悔值）可能越高。

\subsection{方法1：基于 Hoeffding 不等式的动态探索 (\texttt{HoeffdingBasedExploration})}

\paragraph{核心思想：} 利用频率派的 Hoeffding 不等式为每个臂的平均奖励 $\mu_a$ 估计一个置信区间 $[\text{LCB}_a, \text{UCB}_a]$。Hoeffding 不等式提供了一个概率上界，保证真实均值落在基于样本均值 $\hat{\mu}_a$ 构建的置信区间内的概率至少为 $1-\delta$。

\paragraph{Hoeffding 不等式与置信区间推导：}
Hoeffding 不等式表明：
$$P(|\bar{X} - \mu| \ge \epsilon) \le 2e^{-2n\epsilon^2}$$
这表示样本均值 $\bar{X}$ 与真实均值 $\mu$ 的偏差超过 $\epsilon$ 的概率最多为 $2e^{-2n\epsilon^2}$。

为了构建置信区间，我们希望找到一个 $\epsilon$，使得误差大于 $\epsilon$ 的概率至多为某个小值 $\delta$。令概率上界等于 $\delta$：
$$2e^{-2n\epsilon^2} = \delta\Rightarrow \displaystyle\epsilon= \sqrt{\frac{\ln(2/\delta)}{2n}}$$

这个 $\epsilon$ 就是置信区间的半径。真实均值 $\mu$ 落在区间 $[\bar{X} - \epsilon, \bar{X} + \epsilon]$ 内的概率至少为 $1-\delta$。

\paragraph{应用于 K 个臂 (联合界)：}
当同时处理 $k$ 个臂时，我们希望置信保证对所有臂都成立。常用的方法是联合界（Union Bound）。如果我们希望 $k$ 个置信区间中\emph{任何一个}未能包含其真实均值的总概率最多为 $\delta$，我们可以要求每个单独区间的失败概率最多为 $\delta/k$。将 $\epsilon$ 公式中的 $\delta$ 替换为 $\delta/k$ 得到：
$$\epsilon_a(t, \delta) = \sqrt{\frac{\ln(2k/\delta)}{2n_a(t)}}$$
其中 $n_a(t)$ 是臂 $a$ 在时间 $t$ 之前被拉动的次数。
\paragraph{实现细节：}
\begin{itemize}
    \item \textbf{置信区间：} 对于被拉动 $n_a(t)$ 次、样本均值为 $\hat{\mu}_a(t)$ 的臂 $a$，其置信区间为 $[\text{LCB}_a, \text{UCB}_a]$，其中 LCB$_a = \hat{\mu}_a(t) - \epsilon_a(t, \delta)$ 且 UCB$_a = \hat{\mu}_a(t) + \epsilon_a(t, \delta)$。
    \item \textbf{停止条件：} 如果找到一个臂 $a^*$，使得其置信下界 LCB$_{a^*}$ 严格大于所有其他臂 $a \neq a^*$ 的置信上界 UCB$_a$，则停止探索。即：
    $$\hat{\mu}_{a^*}(t) - \epsilon_{a^*}(t, \delta) > \max_{a \neq a^*} \{ \hat{\mu}_a(t) + \epsilon_a(t, \delta) \}$$
    这保证了臂 $a^*$ 在统计上优于所有其他臂，置信度至少为 $1-\delta$。
    \item \textbf{探索策略（未停止时）：} 当停止条件未满足时，选择当前 UCB 值最高的臂进行探索：$\arg\max_a \{\hat{\mu}_a(t) + \epsilon_a(t, \delta)\}$。这旨在优先探索那些潜在最优或不确定性高的臂。
    \item \textbf{参数 $\delta$：} 将 $\delta$ 设置为与 $N$ 相关，例如 $\delta = \frac{1}{N \cdot \text{factor}}$ [cite: Project1/strategies.py]，将置信水平与承诺阶段的长度联系起来。更大的 $N$ 需要更高的置信度（更小的 $\delta$）。
    \item \textbf{理论保证：} 当 $\delta=1/N$ 时，错误识别最优臂的概率 $P(\text{选择次优臂}) \le 1/N$。达到停止条件所需的期望探索轮数大致为 $\Omega(\sum_{a \neq a^*} \frac{\log(kN/\delta)}{\Delta_a^2})$，其中 $\Delta_a = \mu_{a^*} - \mu_a$ 是最优臂与次优臂的真实均值差距。
\end{itemize}

\subsection{方法2：基于贝叶斯后验概率的动态探索 (\texttt{BayesianExploration})}

\paragraph{核心思想：} 采用贝叶斯方法，为每个臂的平均奖励 $\mu_a$ 维护一个后验概率分布。当某个臂是最优臂的后验概率超过一个阈值时，停止探索。

\paragraph{实现细节：}
\begin{itemize}
    \item 使用蒙特卡洛方法评估每个臂是最优臂的后验概率。
    \item 当某臂的后验概率超过阈值($1-1/\N$)时停止探索。
\end{itemize}
三页纸写不下了，略。
\section{实验结果与分析}
\subsection{regret测试}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/single_run_Hoeffding_δ_1_0_N___T_1000__BestEmpirical.png}
        \caption{基于Hoeffding的算法}
        \label{fig:image5}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{fig/comparison_final_regret_T100_N500_k20.png}
        \caption{最终regret比较}
        \label{fig:image6}
    \end{subfigure}
    \caption{算法结果比较}
    \label{fig:both_images2}
\end{figure}
\subsection{}
\end{document}